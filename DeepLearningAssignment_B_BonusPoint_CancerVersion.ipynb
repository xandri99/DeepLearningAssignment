{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3 part 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task HPV slides"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the HPV dataset I want you to design a model that can detect if a sample is from a tumor or not. \n",
    "\n",
    "Choose a neural network you think will work and try to get above 80% accuracy\n",
    "\n",
    "Choose a neural network you do not think will work, run it atleast once. \n",
    "\n",
    "None of the neworks can be a Multi Layer Perceptron.\n",
    "\n",
    "There's a whole thesis written based on this dataset, available here: http://uu.diva-portal.org/smash/get/diva2:1650957/FULLTEXT01.pdf . The thesis then resulted in a paper: https://openaccess.thecvf.com/content_ICCV_2017_workshops/w1/html/Wieslander_Deep_Convolutional_Neural_ICCV_2017_paper.html \n",
    "\n",
    "**You are not allowed to use the exact same network and configurations as in these texts, or as in your hand in for the mandatory assignment, but you are allowed to use the same base network if you are using transfer learning.**  \n",
    "\n",
    "You may discuss theory with other groups, but not code nor share code. You may not use ChatGPT4 or similar programs to generate a solution or code.\n",
    "\n",
    " <span style=\"color:red\"> Solutions deemed to similar will be run through a program designed to detect coding plagirism and if flagged you will be reported for plagirism. </span> \n",
    " \n",
    " One hand in by the group.\n",
    "\n",
    "## Hand-in\n",
    "\n",
    "To get the bonus point you need to turn in a runable notebook that contains:\n",
    "1. A 1-5 scentence motivation of why you chose your networks with references to sources describing the network type.  \n",
    "\n",
    "2. a) A neural network that gets atleast an 80% accuracy   \n",
    "   \n",
    "    b) If you do not reach this whithin a reasonable time frame; the network you have and a well motivated analysis of what could be done differently to improve the accuracy of your model. (Atleast 3 improvement strategies, 1-5 paragraphs) These cannot be the same strategies you used in the mandatory assignment. \n",
    "\n",
    "\n",
    "3. A badly performing network run atleast once. Motivate why this network was not as good a choice as the one you choose. Include references to the source of the network if you're using a pre existing network (1 paragraph)\n",
    "\n",
    "The first, good neural network is the focus of this bonus assignment and you will choose how to best allocate your time\n",
    "\n",
    "For the bad network I wouldn't recommend spending more than an hour on this unless you really, really want.\n",
    "\n",
    "4. Evaluate your best network using atleast 3 metrics and discuss their differences/similarities in relation to your results.  (1-3 paragraphs)\n",
    "\n",
    "Optional: Make a leave-one-out analysis of your best network to show that you weren't simply lucky in how you divided your data set. \n",
    "\n",
    "## Bonus points\n",
    "1 bonus point: 1, 2 a *or* b, and 3 all answered correctly \n",
    "1 bonus point: 4 answered and discussed\n",
    "\n",
    "So for 2 bonus points: 1, 2a *or* 2b, 3, 4 all answered correctly "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q1. A 1-5 scentence motivation of why you chose your networks with references to sources describing the network type.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# T1. A neural network that gets atleast an 80% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test\n",
      "H    1572\n",
      "T     746\n",
      "Name: classification, dtype: int64\n",
      "Train\n",
      "H    5153\n",
      "T    2341\n",
      "Name: classification, dtype: int64\n",
      "Validation\n",
      "H    1270\n",
      "T     588\n",
      "Name: classification, dtype: int64\n",
      "Shape of the images is:  (48, 48)\n"
     ]
    }
   ],
   "source": [
    "## IMPORTS\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import IPython\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from datetime import datetime\n",
    "import cv2\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "#VGG16\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "#ResNet\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "\n",
    "#EfficientNet\n",
    "from tensorflow.keras.applications import EfficientNetB7 as EfficientNet\n",
    "\n",
    "#Lab scripts\n",
    "import plot_helper\n",
    "import cnn_helper\n",
    "\n",
    "\n",
    "\n",
    "## DATA PREPARATION\n",
    "\n",
    "## Directory\n",
    "base_directory = \"/home/jovyan/big-data_deeplearning/\"\n",
    "images_path = base_directory + \"LabData/cell_images_small/\"\n",
    "\n",
    "## Create a Dataframe for each set of images, adding a category column with H for healthy and T for tumor\n",
    "\n",
    "# training dataset\n",
    "for folder in [\"test\", \"train\", \"validation\"]:\n",
    "    data = []\n",
    "    for category in [\"healthy\", \"tumor\"]:\n",
    "        category_path = os.path.join(images_path, folder, category)\n",
    "        for filename in os.listdir(category_path):\n",
    "            imageID = filename\n",
    "            classification = \"H\" if category == \"healthy\" else \"T\"\n",
    "            data.append({\"imageID\" : imageID, \"classification\" : classification})\n",
    "    if folder == \"test\":\n",
    "        df_test = pd.DataFrame(data)\n",
    "        # see the weights of each class\n",
    "        print(\"Test\")\n",
    "        print(df_test['classification'].value_counts())\n",
    "    if folder == \"train\":\n",
    "        df_train = pd.DataFrame(data)\n",
    "        # see the weights of each class\n",
    "        print(\"Train\")\n",
    "        print(df_train['classification'].value_counts())\n",
    "    if folder == \"validation\":\n",
    "        df_validation = pd.DataFrame(data)\n",
    "        # see the weights of each class\n",
    "        print(\"Validation\")\n",
    "        print(df_validation['classification'].value_counts())\n",
    "            \n",
    "\n",
    "## Check image shape\n",
    "photo = Image.open(\"/home/jovyan/big-data_deeplearning/LabData/cell_images_small/test/healthy/cell_10088.tif\")\n",
    "array_photo = np.array(photo)\n",
    "print(\"Shape of the images is: \", array_photo.shape)\n",
    "image_shape = array_photo.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras_preprocessing/image/dataframe_iterator.py:279: UserWarning: Found 1 invalid image filename(s) in x_col=\"imageID\". These filename(s) will be ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7493 validated image filenames belonging to 2 classes.\n",
      "Found 1858 validated image filenames belonging to 2 classes.\n",
      "Found 2317 validated image filenames belonging to 2 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/keras_preprocessing/image/dataframe_iterator.py:279: UserWarning: Found 1 invalid image filename(s) in x_col=\"imageID\". These filename(s) will be ignored.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "## DATA GENERATOR\n",
    "\n",
    "## Directory with all the images:\n",
    "train_path = \"/home/jovyan/big-data_deeplearning/LabData/cell_images_small/train/all/\"\n",
    "test_path = \"/home/jovyan/big-data_deeplearning/LabData/cell_images_small/test/all/\"\n",
    "validation_path = \"/home/jovyan/big-data_deeplearning/LabData/cell_images_small/validation/all/\"\n",
    "\n",
    "## Set up generators for all datasets\n",
    "image_shape = (48, 48, 3)\n",
    "\n",
    "batch_size = 32\n",
    "filename_column = \"imageID\"\n",
    "true_label_column = \"classification\"\n",
    "\n",
    "## Doing the preprocessing required\n",
    "train_data_generator = keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "valid_data_generator = keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "test_data_generator = keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "\n",
    "train_generator = train_data_generator.flow_from_dataframe(\n",
    "    df_train, directory=train_path, x_col=filename_column, y_col=true_label_column,\n",
    "    class_mode='binary', batch_size=batch_size, target_size = image_shape[:2], color_mode='rgb', shuffle=False)\n",
    "\n",
    "valid_generator = valid_data_generator.flow_from_dataframe(\n",
    "    df_validation, directory=validation_path, x_col=filename_column, y_col=true_label_column,\n",
    "    class_mode='binary', batch_size=batch_size, target_size = image_shape[:2], color_mode='rgb', shuffle=False)\n",
    "\n",
    "\n",
    "test_generator = test_data_generator.flow_from_dataframe(\n",
    "    df_test, directory=test_path, x_col=filename_column, y_col=true_label_column,\n",
    "    class_mode='binary', batch_size=batch_size, target_size = image_shape[:2], color_mode='rgb', shuffle=False)\n",
    "\n",
    "\n",
    "train_steps = train_generator.n//train_generator.batch_size if train_generator.n >= train_generator.batch_size else 1\n",
    "validation_steps = valid_generator.n//valid_generator.batch_size if valid_generator.n >= valid_generator.batch_size else 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 860, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 918, in compute_loss\n        return self.compiled_loss(\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/losses.py\", line 141, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/losses.py\", line 245, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/losses.py\", line 1932, in binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    File \"/usr/local/lib/python3.8/dist-packages/keras/backend.py\", line 5247, in binary_crossentropy\n        return tf.nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)\n\n    ValueError: `logits` and `labels` must have the same shape, received ((None, 2) vs (None, 1)).\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m## Actually train model\u001b[39;00m\n\u001b[1;32m     26\u001b[0m epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m---> 27\u001b[0m EfficientNet_history \u001b[38;5;241m=\u001b[39m \u001b[43mEfficientNet_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m                    \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvalid_generator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mvalidation_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/func_graph.py:1147\u001b[0m, in \u001b[0;36mfunc_graph_from_py_func.<locals>.autograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint:disable=broad-except\u001b[39;00m\n\u001b[1;32m   1146\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mag_error_metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1147\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mag_error_metadata\u001b[38;5;241m.\u001b[39mto_exception(e)\n\u001b[1;32m   1148\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1149\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1021, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1010, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 1000, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 860, in train_step\n        loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/training.py\", line 918, in compute_loss\n        return self.compiled_loss(\n    File \"/usr/local/lib/python3.8/dist-packages/keras/engine/compile_utils.py\", line 201, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/losses.py\", line 141, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/losses.py\", line 245, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.8/dist-packages/keras/losses.py\", line 1932, in binary_crossentropy\n        backend.binary_crossentropy(y_true, y_pred, from_logits=from_logits),\n    File \"/usr/local/lib/python3.8/dist-packages/keras/backend.py\", line 5247, in binary_crossentropy\n        return tf.nn.sigmoid_cross_entropy_with_logits(labels=target, logits=output)\n\n    ValueError: `logits` and `labels` must have the same shape, received ((None, 2) vs (None, 1)).\n"
     ]
    }
   ],
   "source": [
    "## Instantiate the base model EfficientNet\n",
    "EfficientNet_base_model = EfficientNet(weights = 'imagenet', input_shape = image_shape, include_top = False) \n",
    "\n",
    "## Freeze the base model, so that we don't modify the weights when backpropagating\n",
    "EfficientNet_base_model.trainable = False\n",
    "\n",
    "# Our top layers, and the model that we will train.\n",
    "last_layer = EfficientNet_base_model.output\n",
    "x = keras.layers.Flatten()(last_layer) # could be GlobalAveragePooling2D, GlobalMaxPooling2D or Flatten as well, not sure how to pick\n",
    "x = keras.layers.Dense(512, activation = 'relu')(x)\n",
    "x = keras.layers.Dropout(0.5)(x)  # To reduce overfittin, since in the plots we are getting a lot.\n",
    "#x = keras.layers.Dense(512, activation = 'relu')(x)\n",
    "x = keras.layers.Dense(2, activation = 'sigmoid')(x)\n",
    "\n",
    "# Ensamble the new full network\n",
    "EfficientNet_model = tf.keras.Model(EfficientNet_base_model.input, x, name = \"EfficientNet_model\")\n",
    "\n",
    "#EfficientNet_model.summary()\n",
    "\n",
    "## Compile the new model, using the base model + the new layers that we added\n",
    "EfficientNet_model.compile(optimizer = keras.optimizers.Adam(), \n",
    "                    loss = keras.losses.BinaryCrossentropy(),\n",
    "                    metrics = [keras.metrics.Accuracy()])\n",
    "\n",
    "## Actually train model\n",
    "epochs = 10\n",
    "EfficientNet_history = EfficientNet_model.fit(train_generator,\n",
    "                    steps_per_epoch = train_steps,\n",
    "                    validation_data = valid_generator,\n",
    "                    validation_steps = validation_steps,\n",
    "                    epochs = epochs,\n",
    "                    batch_size = batch_size,\n",
    "                    verbose = 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"I have some questions about the instructions\"\n",
    "\n",
    "Once you've started to ask yourself questions about the task, read the below and if questions remain ask any TA or me during any lesson or lab. \n",
    "\n",
    "**Wait, where's the code?** ***Nowhere*** , you'll design this solution all on your own ***as per feedback from last years course evaluation. Your answers on the course evaluation will be incorporated into next years teaching, so do fill out the course evaluation***. Do make sure your solution is clear and be extremely clear and indicate when you're answering questions. Make a new notebook. \n",
    "\n",
    "**The thesis is 80 pages long. How are we supposed to read this *and* do the assignment?** I do not expect you to read the full thesis, the article on the other hand is 8 pages which you could atleast skim.\n",
    "\n",
    "**How do I make a new notebook?** Ask any TA or lecturer. If it turns out to be very difficult for a student to generate a notebook in the environment just continue below the answers here. Write a solution header to indicate the start of your code. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is a reasonable time frame?**: You decide. I'd say atleast 4 hours to conclude part 2 successfully, around 1 hour for part 3, but it is your education and you decide how much time to spend on this part. This is for 1 bonus point and more of a challenge than the other assignment. I do not think most groups will be able to make a network that reaches 88%, depending on the classes they choose, nor do I expect all groups to attempt this bonus part. \n",
    "\n",
    "**Can we work with other groups?**: You may discuss theory, but not code nor share code. Solutions deemed to similar will be run through a program designed to detect coding plagirism and if flagged you will be reported for plagirism. \n",
    "\n",
    "**This is so much fun, can we turn in more than 2 networks?**: Turn in one notebook containing only what is asked for and I will grade this. If time allows (and I do find this fun so it's very likely that time will allow) I will give you feedback on your other choices as well. While limiting your answers to only what is asked for is good practice for the exam, if you want to explore things further in a seperate notebook feel free. \n",
    "\n",
    "\n",
    "(*If you're reading this and wondering why I think this question and clarification is needed it's because I've (happily) run into this in the past. I appreciate that some people enjoy their education to the point of exploring beyond the assignments, and I understand that some people have too many things to get done to do everything fun. So you do you.*)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We've tried so hard and we can't get a good result!:** I'm guessing you've spent a fair amount of time on this. Write what you think would work if you had more time, and if you've tried all strategies you can think of pick 3 and describe them and hand that in with the network. \n",
    "\n",
    "\n",
    "**We are split, some of us want to give up, but some don't** Have you spent atleast 2.5 hours on task 2a? Then you can consider yourself done with that part. Move on to 2b. \n",
    "\n",
    "**We don't want to give up!** If you feel like moving on to part 2b is giving up, think of it more like practicing for a job: Some employers will just want you to get things done, some employers want you to spend just a bit of time on something and if it turns out to be a much bigger task than expected then the resource (your time) needed to get it done might not be worth the possible gain of getting it done.  \n",
    "\n",
    "Turning in possible solution strategies might be appreciated. Managing your time is an important life skill. (Do not ask any phd-student for advice on this though and expect us to do what we say, we're not always good role models.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How long is a paragraph?** Preferably around this answers lenght. Punctuation is appreciated, here and in the exam. I've tried to write 1-5 scentences to indicate lenght, but I'd rather have many short sentences than 1 long one. If your scentence is more than 2 lines it needs a punctuation. *(Do as I say, not as I do. I do try to do as I say)*\n",
    "\n",
    "**We got more than 88%! Do we have to write extra improvement strategies?**: Good job! Absolutely not. If you really, *really*, want feedback on them then include them. \n",
    "\n",
    "**Do we have to be able to do this assignment to pass the exam?** This assignment builds on theory from the lectures, and I do expect you to understand the theory for the exam. However, this part is purposefully a more difficult assignment and your ability to complete this will not determine wheter or not you'll pass the deep learning part of the exam.\n",
    "\n",
    "**These runs take forever!** I'd like to remind you that my runs take up to 60 hours. However when I'm prototyping I'm not running the full k-folds I do when I properly test the model, so it only takes around 20 minutes. Is there anything you can do to speed up your runs? While the program runs find something else to do, like planning the rest of your solution or reviewing what you've learned in deep learning so far.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
